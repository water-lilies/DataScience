############tensorflow 1.15 기반 손글씨  MNIST 데이터 셋 CNN 분류 분석  #############
from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from keras.callbacks import ModelCheckpoint,EarlyStopping

import matplotlib.pyplot as plt
import numpy
import os
import tensorflow as tf

# seed 값 설정
seed = 0
numpy.random.seed(seed)
tf.set_random_seed(seed)

# 데이터 불러오기

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255
Y_train = np_utils.to_categorical(Y_train)
Y_test = np_utils.to_categorical(Y_test)

# 컨볼루션 신경망의 설정
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128,  activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# 모델 최적화 설정
MODEL_DIR = './model/'
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)

modelpath="./model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)

# 모델의 실행
history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, batch_size=200, verbose=0, callbacks=[early_stopping_callback,checkpointer])

# 테스트 정확도 출력
print("\n Test Accuracy: %.4f" % (model.evaluate(X_test, Y_test)[1]))

# 테스트 셋의 오차
y_vloss = history.history['val_loss']

# 학습셋의 오차
y_loss = history.history['loss']

# 그래프로 표현
x_len = numpy.arange(len(y_loss))
plt.plot(x_len, y_vloss, marker='.', c="red", label='Testset_loss')
plt.plot(x_len, y_loss, marker='.', c="blue", label='Trainset_loss')

# 그래프에 그리드를 주고 레이블을 표시
plt.legend(loc='upper right')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()



###############tensorflow 2.0으로 손글씨  cifar10(동물 10분류) 데이터 셋 CNN 분류 분석############
import tensorflow as tf

from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt


(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# 정규화 
train_images, test_images = train_images / 255.0, test_images / 255.0

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    # The CIFAR labels happen to be arrays, 
    # which is why you need the extra index
    plt.xlabel(class_names[train_labels[i][0]])
plt.show()


model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

#생성한 모델의 레이어, 출력 모양, 파라미터 개수 등을 체크 합니다.
model.summary()

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.summary()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_images, train_labels, epochs=10, 
                    validation_data=(test_images, test_labels))

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

#테스트셋의 정확률 출력
print(test_acc)
###############tensorflow 2.0으로 패션 MNIST 데이터 셋 분류 분석 ###########################################


import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
 

# 10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 패션 MNIST 데이터셋을 사용
# 이미지는 해상도(28x28 픽셀)  크기의 넘파이 배열이고 픽셀 값은 0과 255 사이입니다
# 레이블(label)은 0에서 9까지의 정수 배열(이미지에 있는 옷의 클래스(class)를 나타냄)
# 훈련하는데 60,000개의 이미지를 사용
# load_data() 함수를 호출하면 네 개의 넘파이(NumPy) 배열이 반환
# train_images와 train_labels 배열은 모델 학습에 사용되는 훈련 세트
# test_images와 test_labels 배열은 모델 테스트에 사용되는 테스트 세트

fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

print("image shape : " , train_images.shape)
print("train dataset  : " , len(train_labels))
pirnt("train label : ", train_labels)
print("test dataset  : " , len(test_labels))
 

#이미지 확인
plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()

 
#데이터 전처리
train_images = train_images / 255.0
test_images = test_images / 255.0
 
#훈련 세트에서 처음 25개 이미지와 그 아래 클래스 이름을 출력 (데이터 포맷이 올바른지 확인)
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()

 

#모델 구성
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

#tf.keras.layers.Dense와 같은 층들의 가중치(parameter)는 훈련하는 동안 학습됩니다.
#첫 번째 층인 tf.keras.layers.Flatten은 2차원 배열(28 x 28 픽셀)의 이미지 포맷을 28 * 28 = 784 픽셀의 1차원 배열로 변환합니다
#손실 함수(Loss function)-훈련 하는 동안 모델의 오차를 측정
#옵티마이저(Optimizer)-데이터와 손실 함수를 바탕으로 모델의 업데이트 방법을 결정
#Metrics-훈련 단계와 테스트 단계를 모니터링하기 위해 사용

 

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=5)   # 훈련 세트에서 약 0.88(88%) 정도의 정확도를 달성

 
#테스트 세트에서 모델의 성능을 비교합니다 
test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
print('\n테스트 정확도:', test_acc)

#테스트 세트의 정확도가 훈련 세트의 정확도보다 조금 낮습니다. 훈련 세트의 정확도와 테스트 세트의 정확도 사이의 차이는 과대적합(overfitting) 때문입니다.
#훈련된 모델을 사용하여 이미지에 대한 예측 만들기
#테스트 세트에 있는 각 이미지의 레이블을 예측
predictions = model.predict(test_images)
print(predictions[0])   #첫 번째 예측값 확인
#10개의 옷 품목에 상응하는 모델의 신뢰도(confidence)를 나타냅니다.
 

#가장 높은 신뢰도를 가진 레이블 찾기
print(np.argmax(predictions[0]))
print(test_labels[0])

 

#10개 클래스에 대한 예측을 모두 그래프로 표현
def plot_image(i, predictions_array, true_label, img):
  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])
  plt.imshow(img, cmap=plt.cm.binary)
  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red' 

  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

 

def plot_value_array(i, predictions_array, true_label):
  predictions_array, true_label = predictions_array[i], true_label[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)
  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')
 

#0번째 원소의 이미지, 예측, 신뢰도 점수 배열을 확인
i = 0
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions, test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions,  test_labels)
plt.show()

 

#13번째 원소의 이미지, 예측, 신뢰도 점수 배열을 확인
i = 12
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions, test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions,  test_labels)
plt.show()
 

#올바르게 예측된 레이블은 파란색이고 잘못 예측된 레이블은 빨강색입니다. 숫자는 예측 레이블의 신뢰도 퍼센트(100점 만점)입니다
num_rows = 5
num_cols = 3
num_images = num_rows*num_cols
plt.figure(figsize=(2*2*num_cols, 2*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_image(i, predictions, test_labels, test_images)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_value_array(i, predictions, test_labels)
plt.show()

 


##########################딥러닝으로 손글씨 이미지 분류 분석 ###############################
from keras.datasets import mnist
from keras.utils import np_utils
import numpy
import sys
import tensorflow as tf

seed = 0
numpy.random.seed(seed)   # seed 값 설정
tf.set_random_seed(seed)

# MNIST 데이터셋 불러오기
(X_train, Y_class_train), (X_test, Y_class_test) = mnist.load_data()
print("학습셋 이미지 수 : %d 개" % (X_train.shape[0]))
print("테스트셋 이미지 수 : %d 개" % (X_test.shape[0])) 

import matplotlib.pyplot as plt                                 # 그래프로 확인
plt.imshow(X_train[0], cmap='Greys')
plt.show()

for x in X_train[0]:                          # 코드로 확인
    for i in x:
        sys.stdout.write('%d\t' % i)
    sys.stdout.write('\n')

# 차원 변환 과정
X_train = X_train.reshape(X_train.shape[0], 784)
X_train = X_train.astype('float64')
X_train = X_train / 255
X_test = X_test.reshape(X_test.shape[0], 784).astype('float64') /255

print("class : %d " % (Y_class_train[0]))               # 클래스 값 확인

# 바이너리화 과정
Y_train = np_utils.to_categorical(Y_class_train, 10)
Y_test = np_utils.to_categorical(Y_class_test, 10)
print(Y_train[0])


from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint,EarlyStopping
import matplotlib.pyplot as plt
import os


# 모델 프레임 설정
model = Sequential()
model.add(Dense(512, input_dim=784, activation='relu'))
model.add(Dense(10, activation='softmax')) 


# 모델 실행 환경 설정
model.compile(loss='categorical_crossentropy',   optimizer='adam',   metrics=['accuracy'])

# 모델 최적화 설정
MODEL_DIR = './model/'
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)

modelpath="./model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)

# 모델의 실행
history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, batch_size=200, verbose=0, callbacks=[early_stopping_callback , checkpointer ])


# 테스트 정확도 출력
print("\n Test Accuracy: %.4f" % (model.evaluate(X_test, Y_test)[1]))

# 테스트셋의 오차
y_vloss = history.history['val_loss']

# 학습셋의 오차
y_loss = history.history['loss']

# 그래프로 표현
x_len = numpy.arange(len(y_loss))
plt.plot(x_len, y_vloss, marker='.', c="red", label='Testset loss')
plt.plot(x_len, y_loss, marker='.', c="blue", label='Trainset loss')

# 그래프에 그리드를 주고 레이블을 표시
plt.legend(loc='upper right')
# plt.axis([0, 20, 0, 0.35])
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')

plt.show()


##########################딥러닝으로 선형회귀 예측 분석 ###############################

from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split
import numpy
import pandas as pd
import tensorflow as tf

seed = 0 
numpy.random.seed(seed)  # seed 값 설정
tf.set_random_seed(seed)

df = pd.read_csv("../dataset/housing.csv", delim_whitespace=True, header=None)
dataset = df.values
X = dataset[:,0:13]
Y = dataset[:,13]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)

model = Sequential()
model.add(Dense(30, input_dim=13, activation='relu'))
model.add(Dense(6, activation='relu'))
model.add(Dense(1))

model.compile(loss='mean_squared_error',   optimizer='adam')
model.fit(X_train, Y_train, epochs=200, batch_size=10)

#flatten() -  데이터 배열이 몇 차원이든 모두 1차원으로 바꿔 읽기 쉽게 해 주는 함수
Y_prediction = model.predict(X_test).flatten()   # 예측 값과 실제 값의 비교
for i in range(10):
    label = Y_test[i]
    prediction = Y_prediction[i]
    print("실제가격: {:.3f}, 예상가격: {:.3f}".format(label, prediction))









#########################ReView##################################################################
퍼셉트론 :
입력값과 활성화 함수를 사용해서 출력 값을 다음으로 넘기기는 가장 작은  인공 신경망 단위
N개의 이진수가 하나의 뉴런을 통과해서 가중합이 0보다 크면 활성화되는 가장 간단한 신경망 구조
초평면(hyperplane)으로 구분되는 두개의 공간을 분리하는 역할 - AND게이트, OR게이트를 만들 수 있다
XOR게이트를 퍼셉트론으로 구현하기 위해서는 좌표 평면 자체에 변화를 주는 은닉층을 만들어서 공간을 왜곡하면 두 영역으로 분류 가능
XOR게이트를 퍼셉트론으로 구현하기 위해서는 두개의 퍼셉트론을 한번에 계산할 수 있어야 합니다.

퍼셉트론 신경망 수행 흐름 :
환경변수 지정(출력뉴런수, 입력데이터에 적용할 가중치, 가중합에 적용될 바이어스, 활성화함수, 은닉층 수) ->신경망 실행-> 결과를 실제값과 비교 -> 계산된 오차를 출력층과 은닉층의 가중치를 수정 (오차 역전파)  -> 신경망 실행 ->....-> 결과 출력 (분류, 예측)
신경망 내부에서 가중치는 오차 역전파 방법을 사용해 수정합니다.

은닉층으로의 출력에 사용될 활성화 함수 : 기울기 소실문제를 해결해주는 ReLU를 주로 사용
출력층에 사용될 활성화 함수 :  Sigmoid(이항분류), softmax(다항분류),  선형회귀(연속형변수의 값 예측) X

[성능 최적화 함수 ]
모든 데이터에 대해서 미분값 계산은 수행 속도 저하가 발생  ->  랜덤하게 추출한 일부 데이터를 사용해서 더 빠르게 수행 (SGD, 확률적 경사 하강법)
모멘텀 -관성을 이용하여 방향을 고려해서 진동 폭을 줄이기 위해 사용 
알엠에스프롭(RMSprop) - 아다그라드 보폭 민감도를 보완한 방법
아담 - 모멘텀+ 알엠에스프롭(RMSprop)

Sequential() - keras에서 제공하는 모델 구조를 구성할수 이는 객체 
add(Dense(출력뉴런수  ,input_dim= , init= , activation=)) : 입력층 -> 은닉층  구조(layer)
add(Dense(출력뉴런수, activation=)) : 은닉층 -> 은닉층 구조(layer)
add(Dense(1, activation=))   은닉층 -> 출력층 구조(layer)
 
1 epoch  : 학습 프로세스에서 모든 샘플에 대해서 한번 실행되는 것  
다중 분류시에 class가 object타입으로  thin, normal, fat ->정수변환(LabelEncoder) -> one-hot-encoding 변환 (keras.utils.np_utils.to_categorial())

Sequential() 를 이용해서 신경망 layer구성 후
compile(loss=오차함수, optimizer=성능최적화 함수, metrics=정확도, 재현율등 측정 함수) 
fit(독립변수데이터 객체, 종속변수 데이터객체,  epochs=,  batch_size=) 학습 

[오차 함수]
mean_squared_error : 평균제곱 오차 
mean_absolute_error : 평균 절대 오차 
mean_absolute_percentage_error :평균 절대 백분율 오차 
mean_squared_logartithmetic_error : 평균제곱  로그 오차 
분류 문제에서 정확률과 재현률 측정  -  categorial_crossentropy(범주형 교차 엔트로피), binary_crossentropy(이항 교차 엔트로피)

train_testsplit(독립변수데이터 객체, 종속변수 데이터객체,  test_size=,  random_state=)  : 학습데이터와 테스트 데이터 분리

학습 결과 모델을 저장 :  save()
학습 결과 모델을 메모리로 로딩 : load_model()

k겹 교차 검증 - 학습데이터(테스트 데이터)가 적을 경우 , 데이터셋을 여러 개로 나누어 하나씩 테스트 셋으로 사용( 전체 데이터를 테스트셋으로 사용 할 수 있음)
sklearn.model_selection.StratifiedKFold(n_splits= , shuffle=,  random_state=)

keras.callbacks.ModelCheckpoint(filepath=,  monitor=,  verbose=, save_best_only=) :Epoch마다 모델의 정확도를 기록 저장 

Epoch마다   생성된 모델 객체에는 배열로 학습 정확도acc,  검증 정확도val_acc, 학습 손실값loss, 검증 손실값 val_loss

과적합이 발생하기 시작됨을 생성된 모델 객체의 테스트의 오차 속성 val_loss 값이 감소하다가 다시 증가하기 시작하면 ...
과적합이 발생하면 신경망 학습 수행 중단 : EarlyStopping(monitor='val_loss', patience=)

Dense Layer마다 출력 뉴런수 설정(제약이 없다)

Dense Layer 구조, 개수등의 설정에 따라 모델의 성능이 달라지므로 , Dense 생성시에 변경가능한 옵션들, Layer수등을 조정해서 튜닝합니다.(하이퍼파라미터 튜닝)
fit()의 기본 로그는 손실값과 정확도가 표시됩니다.
이진분류는 정확도를 통해서 제대로 학습이 되고 있는지  확인 가능
다중 분류는 정확도, 재현율등...학습이 되고 있는지  확인 가능
metrics 는 평가 기준

evalute(독립변수데이터 객체, 종속변수 데이터객체, metrics=) - 모델을 평가,  손실값 및 metrics 값등을 반환























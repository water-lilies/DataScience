12월 23일 연습문제 

티머니 홈페이지(https://www.t-money.co.kr/)에서 이용안내 선택 > 대중교통 통계자료 선택 > 최근 월간 교통카드 통계자료 게시글에서 엑셀 첨부 파일 다운로드합니ㅏㄷ.
'버스정류장별 이용현황', '지하철 노서별 역별 이용현황', '지하철 유무임별 이용현황', '지하철 시간대별 이용현황' 4개의 탭 확인

1. 지하철 시간대별 이용 현황 데이터 시각화
[참고 코드]
import csv
f = open('subwaytime.csv')
data = csv.reader(f)
#헤더(header) 데이터가 2개의 행으로 이루어져 있음
#사용월, 호선명, 역 ID, 역 이름 , AM 4:00부터 다음 날 AM 3:00까지의 시간이 1시간 단위로 구분되어 있음
# 두 번째 행에서는 공백('') 네 개와 승차와 하차가 번갈아 나옴 
next(data)  #헤더 데이터 제외시킴
next(data)
# 각 행의 4번 인덱스부터 마지막까지의 데이터는 정수로 변환
for row in data :
    row[4:] = map(int, row[4:])
    print(row)
for row in data :
    print(row)
   
2. 출근 시간대 (7시 ~9시) 가장 많이 타고 내리는 역 찾기

#10번 인덱스의 데이터만 추출해서 리스트에 저장하고 리스트의 길이와 리스트에 저장된 값을 출력
import csv
f = open('subwaytime.csv')
data = csv.reader(f)
next(data)
next(data)
result = []
for row in data :
    row[4:] = map(int, row[4:])
    result.append(row[10])
print(len(result))
print(result)

import matplotlib.pyplot as plt
plt.bar(range(len(result)), result)
plt.show()

#아침 7시 승차 인원을 정렬해서 막대그래프로 표현
import matplotlib.pyplot as plt
result.sort()  # 오름차순으로 정렬
plt.bar(range(len(result)), result)
plt.show()

#7~9시까지 승차 인원을 합치면
for row in data :
    row[4:] = map(int, row[4:])
    result.append(sum(row[10:15:2]))

3. 밤 11시에 가장 많이 타는 역 찾기


4. titanic data set 을 decision tree로 분류 분석하시오
X = ndf[ ['pclass', 'sex', 'age', 'sibsp', 'parch', 'embarked']]
Y = ndf['survived']


##########################k-means 군집  분석 실습 ####################
import pandas as pd
uci_path='http://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale customers data.csv'
df = pd.read_csv(uci_path, header=0)
print(df.head())
print(df.info())
print(df.describe())

X=df.iloc[ : , : ]

from sklearn import preprocessing
X=preprocessing.StandardScaler().fit(X).transform(X)

from sklearn import cluster
kmeans = cluster.KMeans(init='k-means++', n_clusters=5, n_init=10)

kmeans.fit(X)

cluster_label = kmeans.labels_  #군집분석의 예측값
print(cluster_label)

df ['Cluster']=cluster_label 
print(df.head())
print(df.info())

import matplotlib.pyplot as plt
df.plot(kind='scatter', x='Grocery', y='Frozen', c='Cluster', cmap='Set1', colorbar=False, figsize=(10, 10))
df.plot(kind='scatter', x='Milk', y='Delicassen', c='Cluster', cmap='Set1', colorbar=False, figsize=(10, 10))

plt.show()
plt.close()










##########################Decision Tree 분류 분석 실습 ####################
import pandas as pd
import numpy as np

uci_path='http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data'
df = pd.read_csv(uci_path, header=None)
#print(df.head())
df.columns=['id', 'clump', 'cell_size', 'cell_shape', 'adhesion', 'epithlial', 'bare_nuclei', 'chromation', 'normal_nucleoli', 'mitoses', 'class']
print(df.info())
print(df.describe())
# print(df['bare_nuclei'].unique())  

df['bare_nuclei'].replace('?', np.nan, inplace=True)
df.dropna(subset=['bare_nuclei'], axis=0, inplace=True)
df['bare_nuclei'] = df['bare_nuclei'].astype('int')
print(df.info())
print(df.describe())

# 설명변수 (독립변수)
X =df[['clump', 'cell_size', 'cell_shape', 'adhesion', 'epithlial', 'bare_nuclei', 'chromation', 'normal_nucleoli', 'mitoses']]
# 종속변수 (예측변수, 반응변수)
Y =df['class']

# 설명변수 데이터 정규화
from sklearn import preprocessing 
X = preprocessing.StandardScaler().fit(X).transform(X)

# train:test(7:3)으로 데이터 분리
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=10)
print(X_train.shape)
print(X_test.shape)

#Decision Tree 분류 모델 생성
from sklearn import tree

tree_model = tree.DecisionTreeClassifier(criterion='entropy',  max_depth=5)
#tree_model = tree.DecisionTreeClassifier(criterion='gini',  max_depth=5)
tree_model.fit(X_train, Y_train)

y_predict = tree_model.predict(X_test)  #
print(y_predict[0:10])
print(Y_test.values[0:10])

from sklearn import metrics
tree_matrix = metrics.confusion_matrix(Y_test, y_predict)
print(tree_matrix)

tree_report = metrics.classification_report(Y_test, y_predict)
print(tree_report)








##########################KNN 분류 분석 실습 ####################
import pandas as pd
import seaborn as sns

df = sns.load_dataset('titanic')
print(df.info())   #891개행, 15변수
print(df.head())

#NaN 값이 많은 deck열(변수) 삭제
#embarked와 embark_town 열(변수)는 의미가 동일하므로 embark_town 열 삭제
ndf = df.drop (['deck', 'embark_town'], axis=1)
print(len(ndf))    #?

#age변수의 값이 NaN인 행을 삭제
ndf = ndf.dropna(subset=['age'], how='any', axis=0)
print(len(ndf))

#embared 열의 NaN값을 승선도시 중에서 가장 많이 출연한 데이터 값으로 치환하기
#ndf['embarked'].value_counts(dropna=True).idxmax()
most_freq = ndf['embarked'].value_counts(dropna=True).idxmax()
print(most_freq)

ndf['embarked'].fillna(most_freq, inplace=True)
print(ndf.describe(include='all'))

# 분류 분석에 사용할 변수 선택
# survived, pclass, sex, age, sibsp, parch, embarked 
X = ndf[ ['pclass', 'sex', 'age', 'sibsp', 'parch', 'embarked']]
Y = ndf['survived']

#범주형 데이터를 모델이 인식할 수 있는 숫자형 데이터로 변환  : one-hot encoding
onehot_sex = pd.get_dummies(ndf['sex'])
ndf = pd.concat([ndf, onehot_sex], axis= 1)

onehot_embarked = pd.get_dummies(ndf['embarked'], prefix='town')
ndf = pd.concat([ndf, onehot_embarked], axis= 1)

X = ndf[ ['pclass', 'female', 'male', 'age', 'sibsp', 'parch', 'town_C','town_Q','town_S']]
Y = ndf['survived']

#KNN 분류 분석을 수행하려면 설명변수를 정규화 (평균 0, 표준편차1)
from sklearn import preprocessing 
X = preprocessing.StandardScaler().fit(X).transform(X)
 
# train data: test data 을  7:3으로 데이터 분리
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=10)
print(X_train.shape)
print(X_test.shape)

#KNN 분류 분석으로  모델 생성
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)  
knn.fit(X_train, Y_train)

#학습 데이터로부터 생성된 모델로부터 예측값 생성
y_predict = knn.predict(X_test)

print(y_predict[0:10])
print(Y_test.values[0:10])

from sklearn import metrics
knn_matrix = metrics.confusion_matrix(Y_test, y_predict)
print(knn_matrix)

knn_report = metrics.classification_report(Y_test, y_predict)
print(knn_report)

# 생존 여부를 분류할 때 영향을 주는 변수를 선택해서 
# k(최근접을 몇 개까지 볼 것인지 지정)는 되도록 작은 수를 설정하고 홀수로 설정해서 분류분석을 수행합니다.
# 데이터셋에서 생존자 클래스(생존자, 비생존자)의 데이터 수가  동일하다면 정확률로, 생존자 클래스의 데이터 수가 상이하다면 f1통계량으로 모델의 정확도를 판단한다.
# 통상적으로 k=1일때 overfitting 발생할 가능성이 높습니다









##########################SVM 분류 분석 실습 ####################
from sklearn import svm
svm_model = svm.SVC(kernel = 'rbf')
svm_model.fit(X_train, Y_train)  #학습, 모델 생성

y_predict = svm_model.predict(X_test)
print(y_predict[0:10])
print(Y_test.values[0:10])

from sklearn import metrics
svm_matrix = metrics.confusion_matrix(Y_test, y_predict)
print(svm_matrix)

svm_report = metrics.classification_report(Y_test, y_predict)
print(svm_report)

#SVM 은 데이터의 특성 개수가 작아도 복잡한 결정 경계를 만들어 주는 분류 방식입니다.
#데이터 전처리(dummy변수, 정규화) 가 필요하며, 매개변수(하이퍼파마미터) 설정에 따라 분류 분석의 성능에 영향이 큼















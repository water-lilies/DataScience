



 
############################################################################
https://logins.daum.net/accounts/signinform.do?url=https%3A%2F%2Fwww.daum.net%2F
로그인 버튼 id는  loginBtn
아이디 input태그 id는 id
비빌번호 input태그 id는 inputPwd
top.cafe.daum.net 페이지에서
내 카페 (가입 카페 목록 가져오기)  : ul.list_favorite>liv div.inner_tbl



####################Selenium Chrome Driver 사용 로그인 페이지 가져오기 실습 #############
from selenium import webdriver 
from bs4 import BeautifulSoup

options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--diable-dev-shm-usage')
#options.add_argument('User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64)  AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36')

driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe', options=options)
driver.implicitly_wait(3)
# driver.header_overrides =  {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64)  AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36'}

driver.get('https://www.hanbit.co.kr/member/login.html')

driver.find_element_by_name('m_id').send_keys('한빛 아이디 ')
driver.find_element_by_name('m_passwd').send_keys('한빛 비밀번호 ')

#로그인 버튼 클릭 이벤트 발생시키기
driver.find_element_by_xpath('//*[@id="login_btn"]').click()

driver.get('http://www.hanbit.co.kr/myhanbit/myhanbit.html')
html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')

mileage = soup.select_one(".mileage_section1 > dd > span ").get_text()  #.mileage_section1  span  
ecoin = soup.select_one(".mileage_section2  span ").get_text()
print("마일리지 : "+mileage  )
print("이코인 : "+ecoin )



####################PhantomJS 사용 브라우저 제어 실습 ###########################
url = "http://www.naver.com/"
# PhantomJS 드라이버 추출하기
browser = webdriver.PhantomJS('C://phantomjs-2.1.1/bin/phantomjs.exe')
# 3초 대기하기
browser.implicitly_wait(3)
# URL 읽어 들이기 
browser.get(url)
# 화면을 캡처해서 저장하기
browser.save_screenshot(“./output/Website.png")
# 브라우저 종료하기
browser.quit()

####################Selenium Chrome Driver 사용 브라우저 제어 실습 ###########################
from selenium import webdriver 

chrom_driver = webdriver.Chrome('C:/ chromedriver/chromedriver.exe')
chrom_driver. implicitly_wait(3)
chrom_driver.get("https://google.com")
chrom_driver.save_screenshot(“./output/Website2.png")
chrom_driver.close()





##############네이버 블로그 검색 open api 이용 Scrapying 실습 #######################

import urllib.request
import json 

client_id = "wnOb5dh4iE_t2T_x6fnT"
client_secret = "rb4AjeDbxb"
query="천문"
encText = urllib.parse.quote(query)
num = 100
url = "https://openapi.naver.com/v1/search/blog?query=" + encText +"&display="+str(num)
 
request = urllib.request.Request(url)
request.add_header("X-Naver-Client-Id",client_id)
request.add_header("X-Naver-Client-Secret",client_secret)
response = urllib.request.urlopen(request)
rescode = response.getcode()

if(rescode==200):
    response_body = response.read()
    datalist = json.loads(response_body)
    count = 1
    print('['+query+'에 대한 포털사이트 블로그 글]')
    for data in datalist['items'] :
        print(str(count) + " : "  +data['title'])
        print("["+data['description'] +"]")
        count+=1
else:
    print("Error Code:" + rescode)
 



#######################세션을 이용한 로그인 정보 가져오기#######################################
#이코인,  마일리지 가져오기
BeautifulSoup객체
soup객체의 select_one() 이용 마일리지 이코인
soup객체의 select_one() 이용  이코인 이코인
출력
soup = BeautifulSoup(res.text, "html.parser")
mileage = soup.select_one(".mileage_section1 > dd > span ").get_text()  #.mileage_section1  span  
ecoin = soup.select_one(".mileage_section2  span ").get_text()
print("마일리지 : "+mileage  )
print("이코인 : "+ecoin )


############### 파이썬 매뉴얼을 재귀적으로 다운받는 프로그램#######################
# 파이썬 매뉴얼을 재귀적으로 다운받는 프로그램
# 모듈 읽어 들이기 
from bs4 import BeautifulSoup
from urllib.request import *
from urllib.parse import *
from os import makedirs
import os.path, time, re
# 이미 처리한 파일인지 확인하기 위한 변수 
proc_files = {}
# HTML 내부에 있는 링크를 추출하는 함수 
def enum_links(html, base):
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select("link[rel='stylesheet']") # CSS
    links += soup.select("a[href]") # 링크
    result = []
    # href 속성을 추출하고, 링크를 절대 경로로 변환 
    for a in links:
        href = a.attrs['href']
        url = urljoin(base, href)
        result.append(url)
    return result

# 파일을 다운받고 저장하는 함수 
def download_file(url):
    o = urlparse(url)
    savepath = "./" + o.netloc + o.path
    if re.search(r"/$", savepath): # 폴더라면 index.html
        savepath += "index.html"
    savedir = os.path.dirname(savepath)
    # 모두 다운됐는지 확인
    if os.path.exists(savepath): return savepath
    # 다운받을 폴더 생성
    if not os.path.exists(savedir):
        print("mkdir=", savedir)
        makedirs(savedir)
    # 파일 다운받기 
    try:
        print("download=", url)
        urlretrieve(url, savepath)
        time.sleep(1) # 1초 휴식 
        return savepath
    except:
        print("다운 실패: ", url)
        return None  
      
# HTML을 분석하고 다운받는 함수 
def analyze_html(url, root_url):
    savepath = download_file(url)
    if savepath is None: return
    if savepath in proc_files: return # 이미 처리됐다면 실행하지 않음 
    proc_files[savepath] = True
    print("analyze_html=", url)
    # 링크 추출 --- (※10)
    html = open(savepath, "r", encoding="utf-8").read()
    links = enum_links(html, url)
    for link_url in links:
        # 링크가 루트 이외의 경로를 나타낸다면 무시 
        if link_url.find(root_url) != 0:
            if not re.search(r".css$", link_url): continue
        # HTML이라면
        if re.search(r".(html|htm)$", link_url):
            # 재귀적으로 HTML 파일 분석하기
            analyze_html(link_url, root_url)
            continue
        # 기타 파일
        download_file(link_url)
if __name__ == "__main__":
    # URL에 있는 모든 것 다운받기 
    url = "https://docs.python.org/3.5/library/"
    analyze_html(url, url)





##########################네이버 영화 평점, Review를 다수의 페이지에서 Scrapying실습###########

import requests
from bs4 import BeautifulSoup
import re

movie_title=[]
movie_point=[]
movie_review=[]

for n in range(1, 11):
    req = requests.get('https://movie.naver.com/movie/point/af/list.nhn?page='+str(n))    
    html =req.text
    soup = BeautifulSoup(html, 'html.parser')
    titles = soup.select('.moive')
    points = soup.select('td.title > div > em')
    reviews = soup.select('td.title')    
    for dom in titles :
        movie_title.append(dom.text)
    for dom in points :
        movie_point.append(dom.text)
    for dom in reviews :
        content = dom.contents[6]  
        content = re.sub("[\n\t]", "", content)   
        content = re.sub("신고", "", content)
        movie_review.append(content)

for i  in range(len(movie_title)) :
    print('영화제목:' , movie_title[i])
    print('평점:' , movie_point[i])
    print('리뷰:' , movie_review[i])
    print('-----------------------------------------')


############################REVIEW########################################
urllib - url작업을 위한 여러 모듈을 모은 패키지(python3)
urllib.request -url주소의 문서를 열고 일기 위해 사용하는 모듈 (예)urlopen(url).read() 읽은 데이터가 python실행환경의 메모리로 로드
urllib.parser - url주소의 문서를 구문 분석을 위해 사용
응답코드 - 200 OK, 404 Not Found , 500 Server Error
urllib.error - urllib.request에 의해 발생되는 예외를 포함하고 있는 모듈
urllib.parser.urlencode(파라미터) - url요청 파라미터를 인코딩 함수
urllib.request.urlretrieved(url,  로컬에 저장될 파일 이름) - url주소의 문서를 다운로드해서 로컬 저장

requests 모듈 - 간편하게 HTTP request 보내고, 세션을 유지하기 용이하며,  파라미터 데이터들을 편리하게 인코딩 기능을 제공 모듈
get(), post(), head(), put(), delete()
get(url, headers=, cookies=) 함수로부터 반환 객체 - Response객체
Response객체.request - 요청을 보낸 객체(request객체)에 접근
Response객체.status_code  - 요청에 대한 응답 코드
Response객체.text - 요청에 대한 응답 body내용을 text 변환
Response객체.content - 요청에 대한 응답 body내용을 byte 변환
Response객체.raise_for_status() - 에러 발생(200 OK 응답이 아닌 경우)
Response객체.json()  - 요청에 대한 응답이 JSON이 경우 딕셔너리 형태로 변환

BeautifulSoup - HTML, XML 파싱하는데 사용되는 라이브러리
BeautifulSoup( , 파서) 생성자 함수에 파싱할 문서 객체(string, url)를 인수로 전달해서 파싱 객체를 생성
html.parser - 단순한  html문서 구문분석 파서
lxml - html, xml에 대해서 더 유연하고 빠르게 구문분석 파서
xml - xml문서만 구문분석 파서
html5lib - 구조가 복잡한 html 문서 구문분석 파서
bs.부모노드.자식노드.자식노드. ... 방식으로 계층 구조로 구성된 html문서내에 특정 요소를 접근
bs.find_all(태그명 , {속성명: 값,.....})    - 태그명, 태그와 속성면과 값 등등의 조건에 맞는 모든 태그를 반환하는 함수
bs.find(태그명 , {속성명: 값,.....}) - 태그명, 태그와 속성면과 값 등등의 조건에 맞는 하나의 태그를 반환하는 함수
find()등의 함수로부터 반환되는 객체 타입 bs.element.Tag
bs.element.Tag.attrs 속성으로부터 태그의 모든 속성 추출
[ 태그의 내용 추출]
bs.element.Tag.string, 
bs.element.Tag.text, 
bs.element.Tag.contents, 
bs.element.Tag.strings
bs.element.Tag.stripped_strings 
bs.element.Tag.get_text()

bs.element.Tag.name  - 태그명 추출
bs.element.Tag.parent - 태그의 부모 노드 추출
bs.element.Tag.child - 태그의 자식 노드 추출
bs.element.Tag.

bs.elect() - CSS의 선택자를 조건에 맞는 모든 태그를 반환하는 함수

모든 요소 CSS의 선택자 : *
<p id="p1"></> 태그의  id 속성  선택자 : #p1
<p class="p1"></> 태그의  class 속성  선택자 : .p1
<input type="number"></>  태그의  type 속성이 number 선택자 : input[type='number']
<input type="text|password|button|radio|checkbox|reset|submit|phone|url|color|..."></>



##############################################
프로젝트에 포함될 내용
##############################################
1. 조명, 프로젝트명(분석목적)
2. 조원소개와 역할 (사진포함)
3. 데이터 출처와 전처리전의 데이터 구조  (테이블구조로 샘플수와 속성 설명 포함)
4. 분석 방법, 흐름 소개
5. 시각화 도구와 이유
7. 분석 결과
8. 보완할 점
9. 프로젝트 느낀점(조원별 소감)
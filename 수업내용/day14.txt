####은닉층으로 좌표 평면을 왜곡시켜서 시그모이드 함수 사용해서 XOR 문제 해결 #######
import tensorflow as tf 
import numpy as np

#가중치와 바이어스
w11 = np.array([-2, -2])
w12 = np.array([2, 2])

w2 = np.array([1, 1])

b1 = 3
b2 = -1
b3 = -1

#퍼셉트론
def MLP(x, w, b):
    y = np.sum(w*x) + b
    if y<=0:
        return 0
    else :
        return 1


#NAND게이트
def NAND(x1, x2):
    return MLP(np.array([x1, x2]), w11, b1)

#OR게이트
def OR(x1, x2):
    return MLP(np.array([x1, x2]), w12, b2)

#AND게이트
def AND(x1, x2):
    return MLP(np.array([x1, x2]), w2, b3)

#XOR게이트
def XOR(x1, x2):
    return  AND(NAND(x1,x2), OR(x1, x2))

if __name__ =='__main__' :
    for x in [(0, 0), (0, 1), (1, 0), (1, 1)] :
        y = XOR(x[0], x[1])
        print("입력값 :"+str(x) +"   출력값 :"+ str(y))








######################로지스틱 회귀분석 : 다중 입력값 , tf.sigmoid(), #############
import tensorflow as tf 
import numpy as np

x_data = np.array( [[2,3],[4,3],[6,4],[8,6],[10, 7], [12, 8], [14, 9]] )
y_data= np.array([0, 0, 0, 1, 1, 1,1] ).reshape(7, 1)

#입력값을 placeholder에 저장
X= tf.placeholder(tf.float64, shape=[None, 2])
Y = tf.placeholder(tf.float64, shape=[None, 1])
 
#실행할 때마다 동일한 출력(결과)를 얻기 위한 값 설정
seed = 0
np.random.seed(seed)
tf.set_random_seed(seed)

#임의의 a, b 값을 변수로 정의
a = tf.Variable(tf.random_uniform([2, 1], dtype=tf.float64))  #[2, 1]은 들어오는  값은 2개 나가는 값은 1개
b = tf.Variable(tf.random_uniform([1], dtype=tf.float64))

#시그모이드 함수 방정식 정의
y = tf.sigmoid(tf.matmul(X, a) + b)

#오차 loss 구하는 함수
loss = -tf.reduce_mean(Y*tf.log(y)+(1-Y)*tf.log(1-y))

#학습률
learning_rate = 0.1

# 오차 loss 값이 최소인 값 찾는 식 정의
gradient_descent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)

predicted = tf.cast(y >0.5, dtype=tf.float64)
accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))

#tf.cast() 함수는 형변환(캐스팅) 수행 => 부동소수점형(실수)를 정수형으로 변환할때는 소수점이하 버림
                                                    bool (논리) 자료형을 정수형으로 변환할때는 True는 1, False는 0으로

#텐서플로 이용하여 학습
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())     # 변수들을 메모리에 생성, 초기화
    for step in range(5001):
        a_, b_, loss_, _ = sess.run([a, b, loss, gradient_descent], feed_dict={X:x_data, Y:y_data})
        if step % 500 == 0:
            print("Epoch: %.f , loss=%.4f,  기울기 a1=%.4f, 기울기 a2=%.4f, 절편 b= %.4f" % (step,  loss_,  a_[0], a_[1],  b_))



new_x = np.array([7, 6.]).reshape(1, 2)
new_y = sess.run(y, feed_dict={X:new_x})
print("공부한 시간 : %d, 과외 수업 횟수 : %d" % (new_x[: , 0], new_x[:, 1]))
print("합격 가능성 : %6.2f %%" %  (new_y*100))

######################로지스틱 회귀분석 : 로그 함수를 이용한 오차 구하기#################################
import tensorflow as tf 
import numpy as np

data =[[2, 0], [4, 0], [6, 0], [8, 1], [10, 1], [12, 1], [14, 1]]

x_data = [i[0] for i in data]
y_data = [i[1] for i in data]

#임의의 a, b 값을 변수로 정의
a = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed=0))
b = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed=0))

#시그모이드 함수 방정식 정의
y = 1/ (1+ np.e**(a*x_data+b))

#오차 loss 구하는 함수
loss = -tf.reduce_mean(np.array(y_data)*tf.log(y)+(1-np.array(y_data))*tf.log(1-y))

#학습률 
learning_rate = 0.5

# 오차 loss 값이 최소인 값 찾는 식 정의
gradient_descent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)

#텐서플로 이용하여 학습
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())     # 변수들을 메모리에 생성, 초기화
    for step in range(6001):
        sess.run(gradient_descent)
        if step % 1000 == 0:
            print("Epoch: %.f , loss=%.4f,  기울기 a=%.4f, 절편 b= %.4f" % (step, sess.run(loss), sess.run(a), sess.run(b)))



###################################################################################
tf.Variable 는 클래스의 생성자 함수
(tensorflow.org/guide/variable)

v = tf.Variable(0)


import tensorflow as tf

#상수 정의
a = tf.constant(100) 
b = tf.constant(50)
add_op = a+ b

#변수 v 선언하기
v = tf.Variable(10)
#세션 시작 
sess = tf.Session()
print(sess.run(v))

# 변수 v에 add_op의 결과 대입하기
let_op = tf.assign(v, add_op)

 
# 변수 초기화히기
sess.run(tf.global_variables_initializer()) 
print(sess.run(v))

#계산식 실행
sess.run(let_op) 
print(sess.run(v))

######################최소 제곱 #################################
import numpy as np

# x 값과 y 값
x=[2, 4, 6, 8]
y=[81, 93, 91, 97]

mx = np.mean(x)  # x와 y의 평균값
my = np.mean(y)
print("x의 평균값:", mx)
print("y의 평균값:", my)

divisor = sum([(mx - i)**2 for i in x])  # 기울기 공식의 분모

def top(x, mx, y, my):   # 기울기 공식의 분자
    d = 0
    for i in range(len(x)):
        d += (x[i] - mx) * (y[i] - my)
    return d

dividend = top(x, mx, y, my)   #분자값 계산 저장
print("분모:", divisor)
print("분자:", dividend)

a = dividend / divisor    # 기울기와 y 절편 구하기
b = my - (mx*a)

# 출력으로 확인
print("기울기 a =", a)
print("y 절편 b =", b)


######################오차 계산 : 평균 제곱근 오차 #################################
import numpy as np

#  임의의 기울기와 절편
ab = [3, 76]

data = [[2,81],[4,93],[6,91],[8,97]]

# x 값과 y 값
x= [ i[0] for i in data]
y= [ i[1] for i in data]


#  임의의 기울기와 절편의 모델로부터 예측값 반환 함수
def predict(x):
    return ab[0]*x+ab[1]

# 평균 제곱근 오차 반환 함수
def rmse(p, a):
    return    np.sqrt(((p-a)**2).mean())

predict_result = []

for i in range(len(x)):
    predict_result.append(predict(x[i]))
    print("공부한 시간 =%.f, 실제 점수 =%.f,  예측점수=%.f" % (x[i], y[i], predict_result[i]))

# 평균 제곱근 오차 반환 함수를 이용해서 최종값 구하기
def  rmse_val(predict_result, y) :
    return rmse(np.array(predict_result), np.array(y))

print("rmse  최종값 :"+ str(rmse_val(predict_result, y)))


######################경사하강법 : 미분값이 0인 기울기와 y절편 찾기 (학습률 지정) #################################

import tensorflow as tf

data = [[2,81],[4,93],[6,91],[8,97]]

# x 값과 y 값
x_data= [ i[0] for i in data]
y_data= [ i[1] for i in data]

#임의의 기울기와 y절편의 값으로 변수로 정의
a=tf.Variable(tf.random_uniform([1], 0, 10, dtype=tf.float64, seed=0))   #기울기 범위 0~10
b = tf.Variable(tf.random_uniform([1], 0, 100, dtype=tf.float64, seed=0))   #y절편 범위 0~100

y = a*x_data+b  #1차방정식의 계산식 정의

#오차 계산 (평균 제곱근 오차 공식)
rmse = tf.sqrt(tf.reduce_mean(tf.square(y-y_data)))

#학습률
learning_rate = 0.1

# 오차 rmse 값이 최소인 값 찾는 식 정의
gradient_descent = tf.train.GradientDescentOptimizer(learning_rate).minimize(rmse)

#텐서플로으로 학습
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())     # 변수들을 메모리에 생성, 초기화
    for step in range(2001):
        sess.run(gradient_descent)
        if step % 100 == 0:
            print("Epoch: %.f , RMSE=%.4f,  기울기 a=%.4f, 절편 b= %.4f" % (step, sess.run(rmse), sess.run(a), sess.run(b)))

#Epoch는 입력값에 대해 몇 번 반복 실험했는지를 나타내는 용어


######################경사하강법 : 다중회귀분석 실습#################################
import tensorflow as tf 

data = [[2,0, 81],[4,4, 93],[6,2,91],[8,3,97]]
# x 값과 y 값
x1= [ i[0] for i in data]
x2= [ i[1] for i in data]
y_data= [ i[2] for i in data]


#임의의 기울기와 y절편의 값으로 변수로 정의
a1=tf.Variable(tf.random_uniform([1], 0, 10, dtype=tf.float64, seed=0))   #기울기 범위 0~10
a2=tf.Variable(tf.random_uniform([1], 0, 10, dtype=tf.float64, seed=0))   #기울기 범위 0~10
b = tf.Variable(tf.random_uniform([1], 0, 100, dtype=tf.float64, seed=0))   #y절편 범위 0~100

y = a1*x1+a2*x2+b  # 입력값이 2개인 1차방정식의 계산식 정의

#오차 계산 (평균 제곱근 오차 공식)
rmse = tf.sqrt(tf.reduce_mean(tf.square(y-y_data)))

#학습률
learning_rate = 0.1

# 오차 rmse 값이 최소인 값 찾는 식 정의
gradient_descent = tf.train.GradientDescentOptimizer(learning_rate).minimize(rmse)

#텐서플로 이용하여 학습
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())     # 변수들을 메모리에 생성, 초기화
    for step in range(2001):
        sess.run(gradient_descent)
        if step % 100 == 0:
            print("Epoch: %.f , RMSE=%.4f,  기울기 a1=%.4f, 기울기 a2=%.4f, 절편 b= %.4f" % (step, sess.run(rmse), sess.run(a1), sess.run(a2),sess.run(b)))







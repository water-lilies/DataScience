
###########################################################
#Epoch마다 모델의 정확도를 기록, 저장 (파일 형식 .hdf5) 
# 모델을 저장하기 위햇 ModelCheckpoint함수를 콜백함수로 사용 - 모니터할 값 지정 (loss, acc  ,val_loss, val_acc ), verbose : 정보 표시 상세도 (0, 1, 2) 지정
# save_best_only은 정확도가 이전 학습보다 개선되었을때만 저장하도록 함

from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pandas as pd
import numpy
import tensorflow as tf
import matplotlib.pyplot as plt

seed = 0
numpy.random.seed(seed)  # seed 값 설정
tf.set_random_seed(seed)

df_pre = pd.read_csv('../dataset/wine.csv', header=None)
df = df_pre.sample(frac=1)  #rac = 1 지정은 원본 데이터의 100%를 불러오라는 의미
dataset = df.values
X = dataset[:,0:12]
Y = dataset[:,12]

model = Sequential() # 모델 설정(4개의 은닉층을 만들어 각각 30, 12, 8, 1개의 노드를 주었습니다)
model.add(Dense(30, input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
 
from keras.callbacks import ModelCheckpoint

# 모델 컴파일
model.compile(loss='binary_crossentropy',   optimizer='adam',    metrics=['accuracy'])

# 모델 저장 폴더 설정
MODEL_DIR = './model/'
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)

#테스트 오차는 케라스 내부에서 val_loss, 학습 정확도는 acc, 테스트셋 정확도는 val_acc, 학습셋 오차는 loss로 각각 기록됩니다
# 모델 저장 조건 설정
modelpath="./model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)

# 모델 실행 및 저장
model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200,verbose=0, callbacks=[checkpointer])


#################그래프로 학습 확인하기#############################

from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pandas as pd
import numpy
import tensorflow as tf
import matplotlib.pyplot as plt

seed = 0
numpy.random.seed(seed)  # seed 값 설정
tf.set_random_seed(seed)

df_pre = pd.read_csv('../dataset/wine.csv', header=None)
df = df_pre.sample(frac=0.15)  #rac = 1 지정은 원본 데이터의 100%를 불러오라는 의미
dataset = df.values
X = dataset[:,0:12]
Y = dataset[:,12]

model = Sequential() # 모델 설정(4개의 은닉층을 만들어 각각 30, 12, 8, 1개의 노드를 주었습니다)
model.add(Dense(30, input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
 
from keras.callbacks import ModelCheckpoint

# 모델 컴파일
model.compile(loss='binary_crossentropy',   optimizer='adam',    metrics=['accuracy'])

# 모델 저장 폴더 설정
MODEL_DIR = './model/'
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)

#테스트 오차는 케라스 내부에서 val_loss, 학습 정확도는 acc, 테스트셋 정확도는 val_acc, 학습셋 오차는 loss로 각각 기록됩니다
# 모델 저장 조건 설정
modelpath="./model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)


history = model.fit(X, Y, validation_split=0.33, epochs=3500, batch_size=500)

# y_vloss에 테스트셋(33%)으로 실험 결과의 오차 값을 저장
y_vloss=history.history['val_loss']

# y_acc에 학습셋(67%)으로 측정한 정확도의 값을 저장
y_acc=history.history['acc']

# x 값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시
x_len = numpy.arange(len(y_acc))
plt.plot(x_len, y_vloss, "o", c="red", markersize=3)
plt.plot(x_len, y_acc, "o", c="blue", markersize=3)

plt.show()


###val_loss 값이 증가하면 과적합이 발생한 것으로 판단되며 학습 중단##############################################################
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pandas as pd
import numpy
import tensorflow as tf
import matplotlib.pyplot as plt

seed = 0
numpy.random.seed(seed)  # seed 값 설정
tf.set_random_seed(seed)

df_pre = pd.read_csv('../dataset/wine.csv', header=None)
df = df_pre.sample(frac=0.15)  #rac = 1 지정은 원본 데이터의 100%를 불러오라는 의미
dataset = df.values
X = dataset[:,0:12]
Y = dataset[:,12]

model = Sequential() # 모델 설정(4개의 은닉층을 만들어 각각 30, 12, 8, 1개의 노드를 주었습니다)
model.add(Dense(30, input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
 
# 모델 컴파일
model.compile(loss='binary_crossentropy',   optimizer='adam',    metrics=['accuracy'])


# 모델 저장 폴더 설정
MODEL_DIR = './model/'
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)

#테스트 오차는 케라스 내부에서 val_loss, 학습 정확도는 acc, 테스트셋 정확도는 val_acc, 학습셋 오차는 loss로 각각 기록됩니다
# 모델 저장 조건 설정
modelpath="./model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)

from keras.callbacks import EarlyStopping

# 자동 중단 설정(EarlyStopping() 함수에 모니터할 값과 테스트 오차가 좋아지지 않아도 몇 번까지 기다릴지를 정합니다.)
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)

# 모델 실행
model.fit(X, Y, validation_split=0.2, epochs=2000, batch_size=500, callbacks=[early_stopping_callback])

# 결과 출력
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))


###########tensofow로 bmi 판정 ##################################
import pandas as pd
import numpy as np
import tensorflow as tf

df = pd.read_csv("./dataset/bmi.csv")

#키와 몸무게 정규화
df["height"] = df["height"] / 200
df["weight"] = df["weight"] / 100

#label 컬럼 변환 - thin[1, 0, 0]/normal[0, 1, 0]/fat [0, 0, 1]
bclass = {"thin": [1, 0, 0] , "normal":[0, 1, 0], "fat": [0, 0, 1]}
df["label_pat"] = df["label"].apply(lambda x: np.array(bclass[x]))

#학습데이터와 테스트 데이터 분류
test_df = df[15000:20000]
test_pat= test_df[["weight", "height"]]
test_ans = list(test_df["label_pat"])

X = tf.placeholder(tf.float32, [None, 2]) #키, 몸부게 데이터 담을 placeholder  선언
Y = tf.placeholder(tf.float32, [None, 3])   #정답 레이블 데이터 담을 placeholder  선언

W = tf.Variable(tf.zeros([2, 3])) 
b = tf.Variable(tf.zeros([3])) 

y = tf.nn.softmax(tf.matmul(X, W) + b)  #소프트맥스 회귀 정의
cross_entropy = -tf.reduce_sum(Y * tf.log(y))  #오차함수 - 교차 엔트로피
train= tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)  #경사하강법으로 학습
 
#예측값, 정답률 계산
predict = tf.equal(tf.argmax(y, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))

sess = tf.Session()
sess.run(tf.global_variables_initializer()) 
for step in range(3501):
    i = (step*100) % 14000
    rows = df[i+1:i+1+100]
    x_pat = rows[["weight", "height"]]
    y_ans =  list(rows["label_pat"])
    sess.run(train, feed_dict={X: x_pat  , Y: y_ans })
    if step%500  == 0 :
        cre = sess.run(cross_entropy, feed_dict={X: x_pat  , Y: y_ans })
        acc = sess.run(accuracy , feed_dict={X: test_pat  , Y: test_ans })
        print("Epoch=", step, "오차=", cre, "정확률(평균)=", acc)

###########keras + tensofow로 bmi 판정 ##################################
import pandas as pd
import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.callbacks import EarlyStopping


df = pd.read_csv("./dataset/bmi.csv")

#키와 몸무게 정규화
df["height"] /= 200
df["weight"] /= 100

X = df[["weight", "height"]].as_matrix()

#label 컬럼 변환 - thin[1, 0, 0]/normal[0, 1, 0]/fat [0, 0, 1]
bclass = {"thin": [1, 0, 0] , "normal":[0, 1, 0], "fat": [0, 0, 1]}
Y = np.empty((20000, 3))
for i, v in enumerate(df["label"]):
    Y[i] = bclass[v]
 
#학습데이터 , 테스트 데이터 분리
X_train, Y_train = X[1:15001], Y[1:15001]
X_test, Y_test = X[15001:20001], Y[15001:20001]

model = Sequential()  #모델 객체 생성
model.add(Dense(512, input_shape=(2, )))    #Dense(노드 수 , ....) 층을 의미하는 객체
model.add(Activation('relu'))   # 활성화 함수
model.add(Dropout(0.1))
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.1))
model.add(Dense(3))      # 분류하고 싶은 클래스 수 만큼 출력으로 구성
model.add(Activation('softmax'))  #활성화 함수

model.compile(loss="categorical_crossentropy", optimizer="rmsprop", metrics=['accuracy'])
#모델 훈련
hist = model.fit(X_train, Y_train, batch_size=100, epochs=20, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=2)], verbose=1)



#모델 평가                    
score = model.evaluate(X_test, Y_test)
print("loss=", score[0])
print("accuracy=", score[1])



# weight decay( 가중치 감소) - 학습중 가중치가 큰 것에 대해서 패널티를 부과해 과적합의 위험을 줄이는 방법
# Dropout - 복잡한 신경망에서 가중치 감소만으로 과적합을 피하기 어려운 경우 뉴런의 연결을 임의로 삭제시켜 신호를 전달하지 못하도록 하는 방법
# softmax 회귀 - 입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하여 출력값들의 총합은 항상 1이 되는 특성의 함수
                       분류하고 싶은 클래스 수 만큼 출력으로 구성
                        소프트 맥스 결과값을 One hot encoder의 입력으로 연결하면 가장 큰 값만 True값, 나머지는  False값이 나오게 하여 이용 가능하다
#val_loss는 에포크 힛수가 많아질 수록 감소하다가 다시 증가됨을 보이는 경우, 과적합이 발생한 것입니다.
#학습에 더 이상 개선의 여지가 없을 경우 학습을 종료시키는 콜백함수(수행중인 함수에서 지정된 함수를 호출,되부름)는 EarlyStopping 입니다.
#Dense(출력 뉴런의 수,  input_dim=입력 뉴런의 수,  init= 가중치 초기화 방법,  activation=활성화 함수)
#relu 활성화 함수는 은닉층에 주로 사용
#sigmoid 활성화 함수는 이진 분류 문제에서 출력층에 주로 사용
#softmax  활성화 함수는 다중 클래스 분류 문제에서 출력층에 주로 사용

EarlyStopping (monitor=, min_delta=, patience=, verbose=, mode=)
monitor는 관찰하고자 하는 항목(val_loss, val_acc)
min_delta : 개선되고 있다고 판단하기 위한 최소 변화량
patience : 개선이 없어도 종료하지 않고 개선이 없는 epoch를 얼마나 기다릴 것이지 epoch수를 지정 
verbose : 정보 표시 상세도 (0, 1, 2)
mode : 관찰 항목에 개선이 없다고 판단하기 위한 기준

loss : epoch마다 훈련 손실값
acc :  epoch마다 훈련 정확도 값
val_loss :   epoch마다 검증 손실값
val_acc :  epoch마다  검증 정확도 값



##############폐암수술 환자 예측 분석 모델 #################
# 딥러닝을 구동하는 데 필요한 케라스 함수를 불러옵니다.
from keras.models import Sequential
from keras.layers import Dense


# 필요한 라이브러리를 불러옵니다.
import numpy
import tensorflow as tf

# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.
seed = 0
numpy.random.seed(seed)
tf.set_random_seed(seed)

# 준비된 수술 환자 데이터를 불러들입니다.
Data_set = numpy.loadtxt("./dataset/ThoraricSurgery.csv",delimiter=",")

# 환자의 기록과 수술 결과를 X와 Y로 구분하여 저장합니다.
X = Data_set[:,0:17]
Y = Data_set[:,17]

# 딥러닝 구조를 결정합니다(모델을 설정하고 실행).
model = Sequential()   
# 첫 번째 은닉층에 input_dim을 적어 줌으로써 첫 번째 Dense가 은닉층 + 입력층의 역할을 겸합니다.
# 데이터에서 17개의 값을 받아 은닉층의 30개 노드로 보낸다 
model.add(Dense(30, input_dim=17, activation='relu'))  #activation : 출력층으로 전달할 때 사용할 활성화 함수
model.add(Dense(1, activation='sigmoid'))  #출력층의 노드 수는 1개, 최종 출력 값에 사용될 활성화 함수

# 딥러닝을 실행합니다. (오차 함수 :  평균 제곱 오차 함수 사용)
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=30, batch_size=10)

# 결과를 출력합니다.
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))


####################이항 분류 분석 딥러닝으로 학습 ###############################
from keras.models import Sequential
from keras.layers import Dense
import numpy
import tensorflow as tf

seed = 0
numpy.random.seed(seed)                   # seed 값 생성
tf.set_random_seed(seed)

dataset = numpy.loadtxt("./dataset/pima-indians-diabets.csv", delimiter=",")               # 데이터 로드
X = dataset[:,0:8]
Y = dataset[:,8]

model = Sequential()                                                # 모델의 설정
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',   optimizer='adam',      metrics=['accuracy'])   # 모델 컴파일
model.fit(X, Y, epochs=200, batch_size=10)                                # 모델 실행
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))                # 결과 출력


####################다중  분류 분석 딥러닝으로 학습 ###############################
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df = pd.read_csv(’./dataset/iris.csv’, names = [“sepal_length”, “sepal_width”, “petal_length”, “petal_width”, “species”])
print(df.head())

sns.pairplot(df, hue='species')  #속성별 연관성 파악
plt.show()

dataset = df.values
X = dataset[:,0:4].astype(float)
Y_obj = dataset[:,4]

from sklearn.preprocessing import LabelEncoder     
e = LabelEncoder()     # array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])가 array([1,2,3])로 변환
e.fit(Y_obj)
Y = e.transform(Y_obj)

from keras.utils import np_utils
# array([1,2,3])가 다시 array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])로 원-핫 인코딩(one-hot-encoding) 변환
Y_encoded = np_utils.to_categorical(Y)

from keras.models import Sequential
from keras.layers.core import Dense
import numpy
import tensorflow as tf

seed = 0
numpy.random.seed(seed) # seed 값 설정
tf.set_random_seed(seed)

model = Sequential() # 모델의 설정
model.add(Dense(16, input_dim=4, activation='relu'))
#최종 출력 값이 3개 중 하나여야 하므로 출력층에 해당하는 Dense의 노드 수를 3으로 설정
model.add(Dense(3, activation='softmax'))

# 모델 컴파일(다중 분류에 적절한 오차 함수인 categorical_crossentropy를 사용, 최적화 함수로 adam 사용)
model.compile(loss='categorical_crossentropy',    optimizer='adam',    metrics=['accuracy'])

# 모델 실행(한 번에 입력되는 값은 1개, 전체 샘플이 50회 반복될 때까지 실험을 진행
model.fit(X, Y_encoded, epochs=50, batch_size=1)   

print("\n Accuracy: %.4f" % (model.evaluate(X, Y_encoded)[1]))   # 결과 출력




###초음파 광물 분석########################################################
from keras.models import Sequential
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy
import tensorflow as tf

seed = 0
numpy.random.seed(seed)        # seed 값 설정
tf.set_random_seed(seed)

df = pd.read_csv('./dataset/sonar.csv', header=None)           # 데이터 입력
dataset = df.values
X = dataset[:,0:60]
Y_obj = dataset[:,60]
#print(Y_obj.unique())

e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)            # 문자열 변환

model = Sequential()              # 모델 설정
model.add(Dense(24, input_dim=60, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='mean_squared_error',   optimizer='adam',    metrics=['accuracy'])  # 모델 컴파일

hist = model.fit(X, Y, epochs=200, batch_size=5)                      # 모델 실행
# print(hist)
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))     # 결과 출력


#학습 데이터와 테스트 데이터가 중복되면 과적합이 발생합니다.#########################################
#데이터를 7:3의 비율로 랜덤하게 학습 데이터와 테스트 데이터로 분리해서 모델을 학습시키고 정확도를 측정합니다.
from keras.models import Sequential, load_model
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy
import tensorflow as tf

seed = 0
numpy.random.seed(seed)        # seed 값 설정
tf.set_random_seed(seed)

df = pd.read_csv('./dataset/sonar.csv', header=None)           # 데이터 입력
dataset = df.values
X = dataset[:,0:60]
Y_obj = dataset[:,60]

e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)            # 문자열 변환

# 학습셋과 테스트셋의 구분
X_train, X_test, Y_train, Y_test = train_testsplit(X, Y, test_size=0.3, random_state=seed)

model = Sequential()
model.add(Dense(24, input_dim=60, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='mean_squared_error',    optimizer='adam',    metrics=['accuracy'])
#학습 데이터로 학습
model.fit(X_train, Y_train, epochs=130, batch_size=5)
model.save(‘./output/my_model.h5')   # 학습을 통해 생성된 모델을 컴퓨터에 저장

del model   # 테스트를 위해 메모리 내의 모델을 삭제
model = load_model(‘./output/my_model.h5')   # 모델을 새로 불러옴

print("\n Test Accuracy: %.4f" % (model.evaluate(X_test, Y_test)[1]))   # 불러온 모델로 테스트 실행


####################데이터 샘플이 적은 경우 : k-겹 교차 검증########################################################
# 10개의 파일로 쪼개 테스트하는?10-fold cross validation을 실시하도록?n_fold의 값을 10으로 설정한 뒤?StratifiedKFold()?함수에 적용했습니다. 그런 다음 모델을 만들고 실행하는 부분을?for?구문으로 묶어?n_fold만큼 반복되게 합니다.
from keras.models import Sequential 
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy
import tensorflow as tf

seed = 0
numpy.random.seed(seed)        # seed 값 설정
tf.set_random_seed(seed)

df = pd.read_csv('./dataset/sonar.csv', header=None)           # 데이터 입력
dataset = df.values
X = dataset[:,0:60]
Y_obj = dataset[:,60]

e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)            # 문자열 변환


from sklearn.model_selection import StratifiedKFold

n_fold = 10  #10겹 
skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)

accuracy = []  # 빈 accuracy 배열

for train, test in skf.split(X, Y):  # 모델의 설정, 컴파일, 실행
    model = Sequential()
    model.add(Dense(24, input_dim=60, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='mean_squared_error',  optimizer='adam',   metrics=['accuracy'])
    model.fit(X[train], Y[train], epochs=100, batch_size=5)
    k_accuracy = "%.4f" % (model.evaluate(X[test], Y[test])[1])
    accuracy.append(k_accuracy)

# 결과 출력
print("\n %.f fold accuracy:" % n_fold, accuracy)












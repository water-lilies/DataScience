{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이썬 매뉴얼을 재귀적으로 다운받는 프로그램\n",
    "- 파이썬 매뉴얼을 재귀적으로 다운받는 프로그램\n",
    "- 모듈 읽어 들이기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import *\n",
    "from urllib.parse import *\n",
    "from os import makedirs\n",
    "import os.path, time, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 처리한 파일인지 확인하기 위한 변수 \n",
    "proc_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 내부에 있는 링크를 추출하는 함수 \n",
    "def enum_links(html, base):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.select(\"link[rel='stylesheet']\") # CSS\n",
    "    links += soup.select(\"a[href]\") # 링크\n",
    "    result = []\n",
    "    # href 속성을 추출하고, 링크를 절대 경로로 변환 \n",
    "    for a in links:\n",
    "        href = a.attrs['href']\n",
    "        url = urljoin(base, href)\n",
    "        result.append(url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일을 다운받고 저장하는 함수 \n",
    "def download_file(url):\n",
    "    o = urlparse(url)\n",
    "    savepath = \"./\" + o.netloc + o.path\n",
    "    if re.search(r\"/$\", savepath): # 폴더라면 index.html\n",
    "        savepath += \"index.html\"\n",
    "    savedir = os.path.dirname(savepath)\n",
    "    # 모두 다운됐는지 확인\n",
    "    if os.path.exists(savepath): return savepath\n",
    "    # 다운받을 폴더 생성\n",
    "    if not os.path.exists(savedir):\n",
    "        print(\"mkdir=\", savedir)\n",
    "        makedirs(savedir)\n",
    "    # 파일 다운받기 \n",
    "    try:\n",
    "        print(\"download=\", url)\n",
    "        urlretrieve(url, savepath)\n",
    "        time.sleep(1) # 1초 휴식 \n",
    "        return savepath\n",
    "    except:\n",
    "        print(\"다운 실패: \", url)\n",
    "        return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML을 분석하고 다운받는 함수 \n",
    "def analyze_html(url, root_url):\n",
    "    savepath = download_file(url)\n",
    "    if savepath is None: return\n",
    "    if savepath in proc_files: return # 이미 처리됐다면 실행하지 않음 \n",
    "    proc_files[savepath] = True\n",
    "    print(\"analyze_html=\", url)\n",
    "    # 링크 추출 --- (※10)\n",
    "    html = open(savepath, \"r\", encoding=\"utf-8\").read()\n",
    "    links = enum_links(html, url)\n",
    "    for link_url in links:\n",
    "        # 링크가 루트 이외의 경로를 나타낸다면 무시 \n",
    "        if link_url.find(root_url) != 0:\n",
    "            if not re.search(r\".css$\", link_url): continue\n",
    "        # HTML이라면\n",
    "        if re.search(r\".(html|htm)$\", link_url):\n",
    "            # 재귀적으로 HTML 파일 분석하기\n",
    "            analyze_html(link_url, root_url)\n",
    "            continue\n",
    "        # 기타 파일\n",
    "        download_file(link_url)\n",
    "if __name__ == \"__main__\":\n",
    "    # URL에 있는 모든 것 다운받기 \n",
    "    url = \"https://docs.python.org/3.5/library/\"\n",
    "    analyze_html(url, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세션을 이용한 로그인 정보 가져오기\n",
    "> 이코인,  마일리지 가져오기\n",
    "\n",
    "**BeautifulSoup객체**\n",
    "\n",
    "soup객체의 select_one() 이용 마일리지 이코인\n",
    "\n",
    "soup객체의 select_one() 이용  이코인 이코인\n",
    "\n",
    "출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "mileage = soup.select_one().get_text()\n",
    "ecoin = soup.select_one().get_text()\n",
    "print(\"마일리지 : \"+mileage )\n",
    "print(\"이코인 : \"+ecoin )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"wnOb5dh4iE_t2T_x6fnT\"\n",
    "client_secret = \"rb4AjeDbxb\"\n",
    "encText = urllib.parse.quote(\"천문\")\n",
    "num = 100\n",
    "url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText +\"&display=+str(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "    datalist = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('['+query+'에 대한 포털사이트 블로그 글]')\n",
    "    for data in datalist['items'] :\n",
    "        print(str(count) + \" : \"  +data['title'])\n",
    "        print(\"[\"+data['description'] +\"]\")\n",
    "        count+=1\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 블로그 검색 open api 이용 Scrapying 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json \n",
    "\n",
    "client_id = \"wnOb5dh4iE_t2T_x6fnT\"\n",
    "client_secret = \"rb4AjeDbxb\"\n",
    "query=\"천문\"\n",
    "encText = urllib.parse.quote(query)\n",
    "num = 100\n",
    "url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText +\"&display=\"+str(num)\n",
    " \n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "\n",
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "    datalist = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('['+query+'에 대한 포털사이트 블로그 글]')\n",
    "    for data in datalist['items'] :\n",
    "        print(str(count) + \" : \"  +data['title'])\n",
    "        print(\"[\"+data['description'] +\"]\")\n",
    "        count+=1\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium Chrome Driver 사용 브라우저 제어 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "\n",
    "chrom_driver = webdriver.Chrome('C:/ chromedriver/chromedriver.exe')\n",
    "chrom_driver. implicitly_wait(3)\n",
    "chrom_driver.get(\"https://google.com\")\n",
    "chrom_driver.save_screenshot(“./output/Website2.png\")\n",
    "chrom_driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhantomJS 사용 브라우저 제어 실습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.naver.com/\"\n",
    "# PhantomJS 드라이버 추출하기\n",
    "browser = webdriver.PhantomJS('C://phantomjs-2.1.1/bin/phantomjs.exe')\n",
    "# 3초 대기하기\n",
    "browser.implicitly_wait(3)\n",
    "# URL 읽어 들이기 \n",
    "browser.get(url)\n",
    "# 화면을 캡처해서 저장하기\n",
    "browser.save_screenshot(“./output/Website.png\")\n",
    "# 브라우저 종료하기\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium Chrome Driver 사용 로그인 페이지 가져오기 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--diable-dev-shm-usage')\n",
    "#options.add_argument('User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64)  AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36')\n",
    "\n",
    "driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe', options=options)\n",
    "driver.implicitly_wait(3)\n",
    "# driver.header_overrides =  {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64)  AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36'}\n",
    "\n",
    "driver.get('https://www.hanbit.co.kr/member/login.html')\n",
    "\n",
    "driver.find_element_by_name('m_id').send_keys('한빛 아이디 ')\n",
    "driver.find_element_by_name('m_passwd').send_keys('한빛 비밀번호 ')\n",
    "\n",
    "#로그인 버튼 클릭 이벤트 발생시키기\n",
    "driver.find_element_by_xpath('//*[@id=\"login_btn\"]').click()\n",
    "\n",
    "driver.get('http://www.hanbit.co.kr/myhanbit/myhanbit.html')\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "mileage = soup.select_one(\".mileage_section1 > dd > span \").get_text()  #.mileage_section1  span  \n",
    "ecoin = soup.select_one(\".mileage_section2  span \").get_text()\n",
    "print(\"마일리지 : \"+mileage  )\n",
    "print(\"이코인 : \"+ecoin )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################\n",
    "https://logins.daum.net/accounts/signinform.do?url=https%3A%2F%2Fwww.daum.net%2F\n",
    "로그인 버튼 id는  loginBtn\n",
    "아이디 input태그 id는 id\n",
    "비빌번호 input태그 id는 inputPwd\n",
    "top.cafe.daum.net 페이지에서\n",
    "내 카페 (가입 카페 목록 가져오기)  : ul.list_favorite>liv div.inner_tbl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
